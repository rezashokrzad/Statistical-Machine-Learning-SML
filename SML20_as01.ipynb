{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Students\n",
    "Please fill in your names and S/U-numbers:\n",
    "* Student 1 name, S/U-number:\n",
    "* Student 2 name, S/U-number:\n",
    "* Student 3 name, S/U-number:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "61d21d39ed6e54bf91075d69a0d7e62f",
     "grade": false,
     "grade_id": "cell-4a3707609a8bbfee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Statistical Machine Learning 2020\n",
    "# Assignment 1\n",
    "# Deadline: 7 October 2020\n",
    "\n",
    "## Instructions\n",
    "* You can __work in groups__ (= max 3 people). __Write the full name and S/U-number of all team members in the header above.__\n",
    "* Make sure you __fill in any place that says__ `YOUR CODE HERE` or \"YOUR ANSWER HERE\" __including comments, derivations, explanations, graphs, etc.__ This means that the elements and/or intermediate steps required to derive the answer have to be in the report. (Answers like 'No' or 'x=27.2' by themselves are not sufficient, even when they are the result of running your code.) If an exercise requires coding, explain briefly what the code does (in comments). All figures should have titles (descriptions), axis labels, and legends (if applicable).\n",
    "* Please do not add new cells, __write the answers only in the provided cells__. Before you turn this problem in, make sure everything runs as expected. First, *restart the kernel* (in the menubar, select Kernel$\\rightarrow$Restart) and then *run all cells* (in the menubar, select Cell$\\rightarrow$Run All). The assignment was written in (and we strongly recommend using) Python 3 by using the corresponding Python 3 kernel for Jupyter.\n",
    "* The assignment includes certain cells that contain tests. Most of the tests are marked as *hidden* and are used for automatic grading. NB: These hidden tests do not provide any feedback! There are also a couple of tests / checks that are visible, which are meant to help you avoid basic coding errors.\n",
    "* __Upload reports to Brightspace as a single .ipynb file containing the submitter's S/U-number: 'SML20_as01_&lt;submitter-S/U-number&gt;.ipynb'__, e.g., 'SML20_as01_S123456.ipynb'. For those working in groups, it is sufficient if one team member uploads the solutions.\n",
    "* For any problems or questions, send us an email, or just ask. Email addresses: G.Bucur@cs.ru.nl, Yuliya.Shapovalova@ru.nl, and tomc@cs.ru.nl .\n",
    "\n",
    "## Introduction\n",
    "Assignment 1 consists of 3 exercises:\n",
    "1. Polynomial curve fitting (50 points),\n",
    "2. Gradient descent algorithm (25 points),\n",
    "3. Probability theory (25 points).\n",
    "The assignment also includes an optional exercise (worth 10 bonus points).\n",
    "\n",
    "## Libraries\n",
    "First, we import the basic libraries necessary to develop this assignment. Of course you are free to import further libraries, if required, in the allotted cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "db1fe1783e44c8723d4302b88fdff5f3",
     "grade": false,
     "grade_id": "cell-3b986e21540420a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Necessary imports (for solutions)\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "\n",
    "# Set fixed random seed for reproducibility\n",
    "np.random.seed(2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "99ea64d3cd3e28125bdcfac0e1b0438b",
     "grade": false,
     "grade_id": "cell-23c36baa38e054e8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 1 (weight 50)\n",
    "Consider once more the $M$-th order polynomial \n",
    "\\begin{equation*}\n",
    "y(x;\\mathbf{w}) = w_0 + w_1 x + \\ldots + w_M x^M  = \\sum_{j=0}^M w_j x^j \n",
    "\\label{yxw} \\tag{1}\n",
    "\\end{equation*}\n",
    "\n",
    "### Exercise 1.1\n",
    "Create the function $f(x) = 1 + \\sin(6(x - 2))$. Generate a data set $\\mathcal{D}_{10}$ of 10 noisy observations of this function. Take the 10 inputs spaced uniformly in range $[0,1]$, and assume that the noise is Gaussian with mean 0 and standard deviation 0.3. $\\mathcal{D}_{10}$ will be the training set. In a similar way, generate an additional test set $\\mathcal{T}$ of 100 noisy observations over the same interval. Plot both the function and observations in $\\mathcal{D}_{10}$ in a single graph (similar to Bishop, Fig.1.2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "26255c98d2140df6b3509daaf853b830",
     "grade": false,
     "grade_id": "cell-b828ba4e85011481",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    \"\"\"\n",
    "    This function computes (x)=1+sin(6(xâˆ’2))\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : float\n",
    "        Input number.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Result of the function.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "03b2e5359ad4e24acb780230e9eb492e",
     "grade": true,
     "grade_id": "cell-7f16a8d5d13e2c74",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Basic check that function f is correct.\n",
    "\"\"\"\n",
    "assert f(2) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9713098967ee5f9f20e9a4e39ce9517f",
     "grade": false,
     "grade_id": "cell-26a157de86b14a7e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate a data set of N_train noisy observations of the function f. Take the inputs spaced uniformly\n",
    "in range [0,1], and add Gaussian noise with mean 0 and standard deviation 0.3.\n",
    "\n",
    "Variable names\n",
    "--------------\n",
    "N_train : int\n",
    "    number of training observations\n",
    "X_train : array\n",
    "    N_train x 1 vector of x-coordinates, uniformly distributed on [0, 1]\n",
    "t_train : array\n",
    "    N_train x 1 vector with corresponding t-values, adding Gaussian noise\n",
    "D_train : matrix\n",
    "    N_train x 2 matrix, the training data created from X_train and t_train\n",
    "\n",
    "N_test : int\n",
    "    number of data points for testing\n",
    "X_test : array\n",
    "    N_test x 1 vector of random x-coordinates taken form a uniform distribution\n",
    "t_test : array\n",
    "    N_test x 1 vector with corresponding t-values, adding Gaussian noise\n",
    "D_test : matrix\n",
    "    N_test x 2 matrix, the test data created from X_test and t_test\n",
    "\"\"\"\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "df60ab3da4b2ddb0c99dae7a38ea1ff9",
     "grade": true,
     "grade_id": "cell-6ed1d922cd47cc3a",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hidden test for variables N_train, X_train, t_train.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f88ec4ee5de76899508d8d160cd3f861",
     "grade": false,
     "grade_id": "cell-90ebf6fa08349b09",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 1.2\n",
    "Create a function `polynomial_curve_fit(D,M)` that takes as input a data set $\\mathcal{D}_{N}$, consisting of $N$ input/output-pairs $\\{x_n,t_n\\}$, and a parameter $M$, representing the order of the polynomial in \\eqref{yxw}, and outputs a vector of weights $\\mathbf{w} = [w_0, \\dots, w_M]$ that minimizes the sum-of-squares error function\n",
    "\\begin{equation*} E(\\mathbf{w}) = \\frac{1}{2} \\sum_{n=1}^N \\{ y(x_{n} ; \\mathbf{w}) - t_{n} \\} ^2 \\tag{2} \\end{equation*}\n",
    "Hint: use the results from the Tutorial Exercises (Week 1, Exercise 5), and `np.linalg.solve` to solve a linear system of equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "67232528866e946551b182e62d30a309",
     "grade": false,
     "grade_id": "cell-b548cc4bd8339def",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def polynomial_curve_fit(D, M):\n",
    "    ''' This functions computes the value of a polynomial with weights w on data points x.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    D : array\n",
    "        Input dataset D.\n",
    "    M : int\n",
    "        The degree of the polynomial.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Fitted weight vector w that minimizes the sum-of-squares function.\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bdfb6fd442c276e88f35c7b53c09aa59",
     "grade": true,
     "grade_id": "cell-a9ef1e5be2f54c31",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hidden test for polynomial_curve_fit.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "370517ee4b238051d00bd6957373241e",
     "grade": false,
     "grade_id": "cell-6178330599de102e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 1.3\n",
    "For the given dataset $\\mathcal{D}_{10}$, run the `polynomial_curve_fit(D,M)` function for $M = [0, \\dots, 9]$,  and, \n",
    "* Plot for various orders $M$ (at least for $M=0, M=1, M=3, M=9$) the resulting polynomial, together with the function $f$ and observations $\\mathcal{D}_{10}$ (similar to Bishop, Fig 1.4)\n",
    "* For each order $M \\in [0, \\dots, 9]$,  compute the root-mean-square error\n",
    "\\begin{equation*} E_{\\text{RMS}} = \\sqrt{2 E(\\mathbf{w^*})/N} \\tag{3} \\end{equation*}\n",
    "of the corresponding polynomial, evaluated on both the training set $\\mathcal{D}_{10}$ and the testset $\\mathcal{T}$.  Plot both as a function of $M$ in a single graph. (see Bishop, Fig.1.5).\n",
    "\n",
    "First define the `polynomial` function to help you with calculating the predictions of outputs for the training and test data given w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e989579b5af6d99d85a83d1322a1d7a4",
     "grade": false,
     "grade_id": "cell-f4213b5c3db7fe63",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def polynomial(x, w):\n",
    "    ''' This functions computes the value of a polynomial with weights w on data points x.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : float\n",
    "        Set of x-coordinates for which to evaluate the polynomial.\n",
    "    w : float\n",
    "        Input weight vector of size M+1 (for polynomial of degree M).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Values of polynomial with weights w evaluated at x.\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1ba5bf14e349d2d4a7334a9dede9e70b",
     "grade": true,
     "grade_id": "cell-ce23788082e731b4",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test for polynomial.\n",
    "\"\"\"\n",
    "assert np.array_equal(polynomial(np.array([1, 2]), np.array([1, 2, 3])),np.array([ 6., 17.]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fdbba7bfadcdcb8ce29ac21c9ac5d281",
     "grade": false,
     "grade_id": "cell-9531c8f8a10afa89",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now with the help of `polynomial` calculate the predictions. Then calculate the root-mean-square-error and \n",
    "create plots for various orders of $M$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a9a127c51fddcda70cf1dc2f871502bc",
     "grade": true,
     "grade_id": "cell-acea12b9aae9969d",
     "locked": false,
     "points": 12,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "29cbc41761f6c4d0e7b5d5cb099b0d88",
     "grade": false,
     "grade_id": "cell-2ba19fdd2a92aba5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 1.4\n",
    "Repeat this procedure for a data set $\\mathcal{D}_{40}$ of  40 observations (with the same noise level) and compare with the previous result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b1b46cd0e8bce40f30a44f289db4e0a2",
     "grade": true,
     "grade_id": "cell-2be39b72eb2e1565",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "718290e0058cb9d15789edf1a1929964",
     "grade": true,
     "grade_id": "cell-d12e3459bbe09e9a",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0f9fe82c2dc2d3d8bcdf6dff71a2d007",
     "grade": false,
     "grade_id": "cell-783fffd6f1da0fc0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 1.5\n",
    "Modify the `polynomial_curve_fit(D,M)` function to include an additional penalty parameter $\\lambda$, for a procedure that solves the minimization problem for a modified error function with quadratic regularizer (weight decay), given as\n",
    "\\begin{equation*}\n",
    "\\tilde{E} = E + \\frac{\\lambda}{2} \\sum_{j=0}^M w_j^2. \\label{regerr} \\tag{4}\n",
    "\\end{equation*}\n",
    "Verify that the regularizer drives the weights of high order terms in the polynomial to zero, and see if you can reproduce and explain the effect observed in Bishop, Fig.1.8. (note that the values here are computed for our data, so they are not identical to the ones in Bishop)\n",
    "\n",
    "|$\\ln\\lambda=$|$-\\infty$ |-18    |-9    |-4   |0    |\n",
    "|-------------|----------|-------|------|-----|-----|\n",
    "|$w_0^*$      |0.87      |0.89   |1.13  |1.59 |1.02 |\n",
    "|$w_1^*$      |-166.39   |25.44  |8.97  |-0.20|-0.38|\n",
    "|$w_2^*$      |4264.93   |-178.33|-29.89|-2.73|-0.41|\n",
    "|$w_3^*$      |-39400.69 |530.85 |11.83 |-1.24|-0.25|\n",
    "|$w_4^*$      |185746.67 |-751.83|15.07 |-0.05|-0.11|\n",
    "|$w_5^*$      |-503783.21|171.74 |3.95  |0.56 |0.01 |\n",
    "|$w_6^*$      |818011.41 |604.53 |-5.68 |0.79 |0.09 |\n",
    "|$w_7^*$      |-785013.15|-111.73|-8.42 |0.83 |0.17 |\n",
    "|$w_8^*$      |410395.73 |-692.14|-3.61 |0.78 |0.21 |\n",
    "|$w_9^*$      |-90055.14 |401.63 |7.65  |0.69 |0.25 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ab6b7a6b20bde2aa55569963bdcf0fc0",
     "grade": false,
     "grade_id": "cell-5156541a32d37722",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def polynomial_curve_fit(D, M, lmb = 0):\n",
    "    ''' This functions computes the value of a polynomial with weights w on data points x.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    D : array\n",
    "        Input dataset D.\n",
    "    M : int\n",
    "        The degree of the polynomial.\n",
    "    lmb : float, optional\n",
    "        Regularization parameter for polynomial curve fitting.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Fitted weight vector w that minimizes the sum-of-squares function.\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "63e8ff74028da08ba4b8aa09174e7933",
     "grade": true,
     "grade_id": "cell-7a30d44592cf4486",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hidden test for polynomial_curve_fit.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8eb6511a30a2a62c7810e358480800ee",
     "grade": true,
     "grade_id": "cell-25af5f44d49bab7b",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "411b098f154bfbec9ba83fd3b2622b23",
     "grade": false,
     "grade_id": "cell-c73e08d307850f6b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 1.6\n",
    "The polynomial curve fitting procedure can be extended to the case of multidimensional inputs. Assuming an input vector  of dimension $D$, namely $\\mathbf{x} = (x_1, x_2, \\dots, x_D)$, we can write the regression function $y$ as:\n",
    "\\begin{equation}\n",
    "y(\\mathbf{x}; \\mathbf{w}) = \\sum_{j = 0}^M \\left( \\sum_{n_1 + n_2 + ... + n_D = j} w_{n_1 n_2 ... n_D} x_1^{n_1} x_2^{n_2} ... x_D^{n_D} \\right) \\label{eqn:polynomial_multidimensional} \\tag{5}\n",
    "\\end{equation}\n",
    "\n",
    "In the last expression, $j$ refers to the order of the polynomial terms. The inner sum is over all the combinations of non-negative integers $n_1, n_2, \\dots, n_D$, such that the constraint $n_1 + n_2 + \\dots + n_D = j$ holds. The terms $n_1, n_2, \\dots, n_D$ correspond to the exponent for each variable $x_1, x_2, \\dots, x_D$ in their respective polynomial term.\n",
    "\n",
    "Note that if $D = 1$, the above expression simplifies to the formula in equation \\eqref{yxw}. The reason the second sum disappears is that there is only one combination of the non-negative integer $n_1$ for which the constraint $n_1 = j$ holds, which means that there is only  a single term to sum over.\n",
    "\n",
    "Fitting the polynomial curve to a multidimensional input vector works analogously to the one-dimensional case. However, the number of parameters (the size of $\\mathbf{w}$) becomes much larger, even when $D = 2$. Write down the general polynomial curve equation in \\eqref{eqn:polynomial_multidimensional} for $D = 2$. How many parameters are needed in the two-dimensional case? Compare this to the number of parameters in the one-dimensional case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "46c3424492c67b2819dfeff34ae69ba3",
     "grade": true,
     "grade_id": "cell-af7fb0f41aed699d",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8fbcefd17285c9b8558c2fed938052e4",
     "grade": false,
     "grade_id": "cell-836d26d54fcf8d85",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 2 (weight 25)\n",
    "In this exercise, we consider the gradient descent algorithm for function minimization. When the function to be minimized is $E(\\mathbf{x})$, the gradient descent iteration is  \n",
    "\\begin{equation*}\n",
    "\\mathbf{x}_{n+1} = \\mathbf{x}_n - \\eta \\nabla E(\\mathbf{x}_n) \\tag{6}\n",
    "\\end{equation*}\n",
    "where $\\eta>0$ is the so-called learning-rate. In the following, we will apply gradient descent to the function\n",
    "\\begin{equation*}\n",
    "h(x,y) = 100(y - x^2)^2 +(1 - x)^2 \\label{banana} \\tag{7}\n",
    "\\end{equation*}\n",
    "### Exercise 2.1\n",
    "Make a plot of the function $h$ over the interval $[-2 \\leq x \\leq 2] \\times [-1 \\leq y \\leq 3]$. (Tip: Use the `plot_surface` function.) Can you guess from the plot if numerical minimization with gradient descent will be fast or slow for this function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "124d3a011b106eb7b5a2fc12fc7c8252",
     "grade": true,
     "grade_id": "cell-5dfbbae25832f037",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "371c859e41a03de5c22ae0fc724183ae",
     "grade": false,
     "grade_id": "cell-d2b407ddc3fbb9fc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create function h.\n",
    "\"\"\"\n",
    "def h(x,y):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\"\"\"\n",
    "Declare x and y.\n",
    "\"\"\"    \n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5e72d8c48f9db03f2234f335cff84737",
     "grade": true,
     "grade_id": "cell-e9cba008bca6b37a",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hiddent test for function h.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2239ad6ded3020c37b621c88c32e1681",
     "grade": true,
     "grade_id": "cell-85ead8c8bd53e79c",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create a function to plot h.\n",
    "\"\"\"\n",
    "def plot_h(x,y):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "plot_h(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4bb88056c5626a0a29bb4c6b368cdc3c",
     "grade": false,
     "grade_id": "cell-31bd1472cf7b6923",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 2.2\n",
    "Knowing that a critical point of a function is a point where the gradient vanishes, show that $(1, 1)$ is the unique critical point of $h$.  Prove that this point is a minimum for $h$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8618eff65f48f8ec036124d062b75a85",
     "grade": true,
     "grade_id": "cell-1336cb1e9d7d38bd",
     "locked": false,
     "points": 6,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9cac0d57868751ba43a415274c7f04b3",
     "grade": false,
     "grade_id": "cell-8f880d3ed9688c1c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 2.3\n",
    "Write down the gradient descent iteration rule for this function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6e7efecd79cbb1883af94d8037de177c",
     "grade": true,
     "grade_id": "cell-91163736650b6698",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d9814f7d9859af71756f720cd7b349d7",
     "grade": false,
     "grade_id": "cell-55ab98d23c58aa29",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 2.4\n",
    "Implement gradient descent. Try some different values of $\\eta$. Does the algorithm converge? How fast? Make plots of the trajectories on top of a contour plot of $h$. (Hint: have a look at the example contour_example.py on Brightspace for inspiration to plot contours of functions and trajectories). Report your findings. Explain why numerical minimization with gradient descent is slow for this function.\n",
    "\n",
    "First implement the derivative of $h(x,y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "61078eb4e5d6d36456e47eef815c91bb",
     "grade": false,
     "grade_id": "cell-ebfab8f71f837375",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def dh_dxy(x, y):\n",
    "    \"\"\"\n",
    "    This function is the derivative of the function h(x, y).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : float\n",
    "        data point from x-axis\n",
    "    y : float\n",
    "        data point from y-axis\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    vals : array\n",
    "        NumPy array of parameter values computed during minimization\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9ebd63a15d17d5c9c196dcafa639bbda",
     "grade": true,
     "grade_id": "cell-14c1d4dd062d9e68",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test for dh_dxy.\n",
    "\"\"\"\n",
    "assert np.array_equal(dh_dxy(1, 1), np.array([0, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3d476333c67e846816f1803ecfa82dd1",
     "grade": false,
     "grade_id": "cell-f8b8e6054e370fc9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now implement the gradient descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bd01bbc0f934f977012cf562398b2fc0",
     "grade": false,
     "grade_id": "cell-e5e7a55f7e7d740f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def grad_descent(grad, val_init, eta, max_iter, tol):\n",
    "    \"\"\" This function implements the gradient descent algorithm.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    grad : function\n",
    "        Returns the derivative of the function with respect to the pair (x, y).\n",
    "    val_init : tuple\n",
    "        Initial values for parameters\n",
    "    eta : float\n",
    "        Gradient descent learning rate\n",
    "    max_iter : int\n",
    "        Maximum number of gradient descent iterations\n",
    "    tol : float\n",
    "        Tolerance for detecting convergence\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    vals : array\n",
    "        NumPy array of parameter values computed during minimization\n",
    "    dists : array\n",
    "        NumPy array of distances from the current point to the previous point\n",
    "    tot_iter : int\n",
    "        Number of performed gradient descent iterations\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e6b956a195e4bca05d7de1b84561f6ae",
     "grade": true,
     "grade_id": "cell-9cce6b818f4aa52e",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hidden test for grad_descent.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9236de9eb8bc43a4cf517390f9a9eb46",
     "grade": false,
     "grade_id": "cell-2829e4b5e8af0502",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Finally, run the gradient descent algorithm with different values of $\\eta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fff6722c457109e005a5829f5226dca6",
     "grade": true,
     "grade_id": "cell-76876feddeb4a5f0",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7634321ff4885a6d2755269999ee081a",
     "grade": false,
     "grade_id": "cell-87fcba196081149d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Explain what you see!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f6dbdf1d1e4690f075aaa40f9de04f02",
     "grade": true,
     "grade_id": "cell-40c451de6dac1163",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "276c481c386c33f499b7a5290c5bca12",
     "grade": false,
     "grade_id": "cell-2a1aae0a18484d04",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 3 (weight 25)\n",
    "Suppose we have two healthy but curiously mixed boxes of fruit, with one box containing 8 apples and 4 grapefruit and the other containing 15 apples and 3 grapefruit. One of the boxes is selected at random and a piece of fruit is picked (but not eaten) from the chosen box, with equal probability for each item in the box. The piece of fruit is returned and then once again from the *same* box a second piece is chosen at random. This is known as sampling with replacement. Model the box by random variable $B$, the first piece of fruit by variable $F_1$, and the second piece by $F_2$.\n",
    "### Exercise 3.1\n",
    "What is the probability that the first piece of fruit is an apple given that the second piece of fruit was a grapefruit? How can the result of the second pick affect the probability of the first pick?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1437c5eef9e60631601e37875c818ba8",
     "grade": true,
     "grade_id": "cell-20711d5530b05594",
     "locked": false,
     "points": 9,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f5081815b1cf92b2880cd47adb5e6cf7",
     "grade": false,
     "grade_id": "cell-49dfe477cdad0ff8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Please add the final result you got in the cell below! (Add it as a fraction, not an estimate. For example, write __1/3__, do not round to a number of decimals.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3a502307711645f0453e6444b8f50156",
     "grade": false,
     "grade_id": "cell-6d4096e0b9231c28",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The variable p is probability of the first piece of fruit being\n",
    "an apple given that the second piece of fruit was a grapefruit.\n",
    "\"\"\"\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4a1787ad7f67e33584a9fea9c6e2281a",
     "grade": true,
     "grade_id": "cell-13f94c0b98af50e6",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hidden check for value of variable p.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "daefb7a66bbaabf1623c232bbcd2e1cd",
     "grade": false,
     "grade_id": "cell-533f0ed93282cea2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 3.2\n",
    "Imagine now that after we remove a piece of fruit, it is not returned to the box. This is known as sampling without replacement. In this situation, recompute the probability that the first piece of fruit is an apple given that the second piece of fruit was a grapefruit. Explain the difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fa99ca592291ca8735ab23dcbf612243",
     "grade": true,
     "grade_id": "cell-c5cabea66205f678",
     "locked": false,
     "points": 9,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2089da2a56d8f63cca56888c8cf0d3cf",
     "grade": false,
     "grade_id": "cell-b2aab5343b969c8a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Please add the final result you got in the cell below! (Add it as a fraction, not an estimate. For example, write __1/3__, do not round to a number of decimals.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "16f923200f4fc392ae85903599942470",
     "grade": false,
     "grade_id": "cell-bbf669c7d7e989eb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The variable p is probability of the first piece of fruit being\n",
    "an apple given that the second piece of fruit was a grapefruit\n",
    "when the sampling was done without replacement.\n",
    "\"\"\"\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "27f560635da3e4ce5e505e55406804ea",
     "grade": true,
     "grade_id": "cell-1aa31b8a1047f19a",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hidden check for value of variable p.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f782f9a85dc09a176d4c28c2aac32fdc",
     "grade": false,
     "grade_id": "cell-5e32edcbb1f96df8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 3.3\n",
    "Starting from the initial situation (i.e., sampling with replacement), we add a dozen oranges to the first box and repeat the experiment. Show that now the outcome of the first pick has no impact on the probability that the second pick is a grapefruit. Are the two picks now dependent or independent? Explain your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1475cbe410865c3142f2505d2c197ae6",
     "grade": true,
     "grade_id": "cell-03574d2eb9eadf55",
     "locked": false,
     "points": 6,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "076b9c2e8fea237dff50a5babbb7a1e7",
     "grade": false,
     "grade_id": "cell-e251ab231093efee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 4 - Bonus (weight 10)\n",
    "Given a joint probability density function over the random vector $\\mathbf{X} = (X_1, X_2, X_3, X_4)$ that factorizes as\n",
    "$$p(x_1, x_2, x_3, x_4) = p(x_1, x_4 | x_2) p(x_2, x_3 | x_1),$$\n",
    "show (using the sum and product rules for marginals and conditionals) that the following independence statements hold, in which the symbol $\\bot$ stands for (conditional) independence:\n",
    "1. $ X_1 \\bot X_2;$\n",
    "2. $ X_3 \\bot X_4 \\,|\\, X_1, X_2.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "46120fe614caf68d61f9324210084709",
     "grade": true,
     "grade_id": "cell-fea97ebe52a73eb3",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
