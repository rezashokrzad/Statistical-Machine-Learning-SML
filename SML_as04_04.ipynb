{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "SML_as04_04.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNIQ-2XajgKH"
      },
      "source": [
        "## Students\n",
        "Please fill in your names and S/U-numbers:\n",
        "* Student 1 name, S/U-number:\n",
        "* Student 2 name, S/U-number:\n",
        "* Student 3 name, S/U-number:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "9b2ebbd8bfd5a074a6de28d01b6674bf",
          "grade": false,
          "grade_id": "cell-e59821d4855bf1fc",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "EOaE1kTDjgKM"
      },
      "source": [
        "# Statistical Machine Learning 2020\n",
        "# Assignment 4\n",
        "# Deadline: 23 December 2020\n",
        "## Instructions\n",
        "* You can __work in groups__ (= max 3 people). __Write the full name and S/U-number of all team members in the header above.__\n",
        "* Make sure you __fill in any place that says__ `YOUR CODE HERE` or \"YOUR ANSWER HERE\" __including comments, derivations, explanations, graphs, etc.__ This means that the elements and/or intermediate steps required to derive the answer have to be in the report. (Answers like 'No' or 'x=27.2' by themselves are not sufficient, even when they are the result of running your code.) If an exercise requires coding, explain briefly what the code does (in comments). All figures should have titles (descriptions), axis labels, and legends (if applicable).\n",
        "* Please do not add new cells unless necessary, try to write the answers only in the provided cells. Before you turn this problem in, __make sure everything runs as expected__. First, *restart the kernel* (in the menubar, select Kernel$\\rightarrow$Restart) and then *run all cells* (in the menubar, select Cell$\\rightarrow$Run All). The assignment was written in (and we strongly recommend using) Python 3 by using the corresponding Python 3 kernel for Jupyter.\n",
        "* The assignment includes certain cells that contain tests. Most of the tests are marked as *hidden* and are used for automatic grading. NB: These hidden tests do not provide any feedback! There are also a couple of tests / checks that are visible, which are meant to help you avoid basic coding errors.\n",
        "* __Upload the exercises to Brightspace as a single .zip file containing the submitter's S/U-number: 'SML20_as04_&lt;S/U-number&gt;.zip'__, for example 'SML20_as04_S123456.zip'. For those working in groups, it is sufficient if one team member uploads the solutions.\n",
        "* For any problems or questions, send us an email, or just ask. Email addresses: G.Bucur@cs.ru.nl, Yuliya.Shapovalova@ru.nl, and tomc@cs.ru.nl.\n",
        "\n",
        "## Introduction\n",
        "Assignment 4 consists of:\n",
        "1. Gaussian processes (50 points);\n",
        "2. EM and doping (50 points);\n",
        "3. Gibbs sampling and Metropolis-Hastings (50 points);\n",
        "4. __Variational inference for Bayesian linear regression (50 points)__.\n",
        "\n",
        "## Libraries\n",
        "First, we import the basic libraries necessary to develop this assignment. Of course you are free to import further libraries, if required, in the allotted cells."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "646b811594a56dc8cdf598e7ad0d6f80",
          "grade": false,
          "grade_id": "cell-eba342b41302b2f1",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "7tVzwDKNjgKP"
      },
      "source": [
        "import IPython\n",
        "assert IPython.version_info[0] >= 3, \"Your version of IPython is too old, please update it to at least version 3.\"\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "import functools\n",
        "\n",
        "# Set fixed random seed for reproducibility\n",
        "np.random.seed(2020)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "4a8950b9d73e22288e3ecd71db2ead33",
          "grade": false,
          "grade_id": "cell-11854439b0684ab8",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "eSDUKZH4jgKS"
      },
      "source": [
        "## Variational inference for Bayesian linear regression\n",
        "In this assignment we will consider variational inference for Bayesian linear regression. Using variational inference we can find an approximate posterior distribution of the parameters of interest. While generally there is no need for variational inference in linear regression problems, it is useful to derive it and implement it for a better understanding.\n",
        "\n",
        "Recall the likelihood function for $\\mathbf{w}$\n",
        "\\begin{equation}\n",
        "p(\\mathbf{t}|\\mathbf{w}) = \\prod_{n=1}^{N}N(t_{n}|\\mathbf{w}^{T}\\phi_{n}, \\beta^{-1}) \n",
        "\\end{equation}\n",
        "and the prior over $\\mathbf{w}$\n",
        "\\begin{equation}\n",
        "p(\\mathbf{w}|\\alpha) = N(\\mathbf{w}|0, \\alpha^{-1}\\mathbf{I}),\n",
        "\\end{equation}\n",
        "where $\\phi_{n}=\\phi(\\mathbf{x_{n}})$. <br>\n",
        "We take a gamma prior distribution for $\\alpha$\n",
        "\\begin{equation}\n",
        "p(\\alpha)=Gam(\\alpha|a_{0}, b_{0}).\n",
        "\\end{equation}\n",
        "Assume $\\beta = 0.01$ to be known and fix it at its 'true value'.\n",
        "\n",
        "### Let us create a simulated data set.\n",
        "We generate the data from a function $\\sin(2\\pi x)$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "54217d08d71b077b5479e20e8b56aa7b",
          "grade": false,
          "grade_id": "cell-dfef00f7ce5ae424",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "DGdgFLjKjgKT"
      },
      "source": [
        "def create_toy_data(func, sample_size, std, domain=[0, 1]):\n",
        "    x = np.linspace(domain[0], domain[1], sample_size)\n",
        "    np.random.shuffle(x)\n",
        "    t = func(x) + np.random.normal(scale=std, size=x.shape)\n",
        "    return x, t\n",
        "\n",
        "def cubic(x):\n",
        "    return x * (x - 5) * (x + 5)\n",
        "\n",
        "def PolynomialFeature(x,degree):\n",
        "    if x.ndim == 1:\n",
        "        x = x[:, None]\n",
        "    x_t = x.transpose()\n",
        "    features = [np.ones(len(x))]\n",
        "    for degree in range(1, degree + 1):\n",
        "        for items in itertools.combinations_with_replacement(x_t, degree):\n",
        "            features.append(functools.reduce(lambda x, y: x * y, items))\n",
        "    return np.asarray(features).transpose()\n",
        "\n",
        "x_train, y_train = create_toy_data(cubic, 10, 10., [-5, 5])\n",
        "x = np.linspace(-5, 5, 100)\n",
        "y = cubic(x)\n",
        "\n",
        "feature = PolynomialFeature(x,degree=3)\n",
        "X_train = PolynomialFeature(x_train, degree=3)\n",
        "X = PolynomialFeature(x, degree=3)\n",
        "\n",
        "plt.scatter(x_train, y_train, s=100, facecolor=\"none\", edgecolor=\"b\")\n",
        "plt.plot(x, y, c=\"g\", label=\"$\\sin(2\\pi x)$\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "4da3a118d994078151f63bd36820865a",
          "grade": false,
          "grade_id": "cell-a4dcf528321af16b",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "DlWXVLgFjgKV"
      },
      "source": [
        "1. Write down the joint distribution of all of the variables $p(\\mathbf{t}, \\mathbf{w}, \\alpha)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "f8e120255f2478a086ea77f53514378d",
          "grade": true,
          "grade_id": "cell-db10d00be8a351dc",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "ZPJtsbQDjgKV"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "6553ca3cd50e2ee2a2e90abc9cbce89e",
          "grade": false,
          "grade_id": "cell-32bed75213fd0ef8",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "7aVuZY9BjgKW"
      },
      "source": [
        "2. Using the variational framework we would like to find an approximation for the posterior distribution $p(\\mathbf{w}, \\alpha|\\mathbf{t})$. Write down the steps (parameter updates at every iteration) for the variational inference algorithm in the case of Bayesian linear regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "4d8735c4e1a073fb0ad333258e4d0dc9",
          "grade": true,
          "grade_id": "cell-5183c55edc34311b",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "Fxp9f997jgKX"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "56297e291ec0e6d653a512be3e3ccaa4",
          "grade": false,
          "grade_id": "cell-941e325937519f95",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "rKGdLDlfjgKY"
      },
      "source": [
        "3. Implement the variational inference algorithm for Bayesian linear regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "543c8997acee10ed251ca2210c54260b",
          "grade": true,
          "grade_id": "cell-d602890eb477cbf5",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "LCI3V9aAjgKZ"
      },
      "source": [
        "def VariationalRegression(X, t, beta, a0, b0, iter_max:int=100):\n",
        "    \"\"\"\n",
        "    Variational Bayesian estimation for linear regression.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : (N, M) np.ndarray\n",
        "        training independent variable\n",
        "    t : (N,) np.ndarray\n",
        "        training dependent variable\n",
        "    beta : float\n",
        "        precision of observation noise (assumed to be known)\n",
        "    a0 : float\n",
        "        a parameter of prior gamma distribution Gamma(alpha|a0,b0)\n",
        "    b0 : float\n",
        "        another parameter of prior gamma distribution Gamma(alpha|a0,b0)    \n",
        "    iter_max : int, optional\n",
        "        maximum number of iteration (the default is 100)\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    w_mean : mean of the variational posterior of w\n",
        "    w_variance: covarioance of the variational posterior of w\n",
        "    a: parameter of the variational posterior of alpha\n",
        "    b: parameter of the variational posterior of beta\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "d8c414a6c25f0bf86ae9d303d8950dad",
          "grade": false,
          "grade_id": "cell-95d1149f51f5ff44",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "4GmAi2o8jgKa"
      },
      "source": [
        "4. Implement the predictive distribution over $t$ given a new data point ($x$) (see equation 10.105) in Bishop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "b3f720382f0a550f6723f10a3aee4f59",
          "grade": true,
          "grade_id": "cell-a0743593ba4d4984",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "_ozdSb6QjgKb"
      },
      "source": [
        "def predict(X, w_mean, w_variance):\n",
        "        \"\"\"\n",
        "        Make a prediction based on the input.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : (N, D) np.ndarray\n",
        "            independent variable\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y : (N,) np.ndarray\n",
        "            mean of predictive distribution\n",
        "        y_std : (N,) np.ndarray\n",
        "            standard deviation of predictive distribution\n",
        "        \"\"\"\n",
        "        # YOUR CODE HERE\n",
        "        raise NotImplementedError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "1439efb54012f2b5ec423e77ac6ca13d",
          "grade": false,
          "grade_id": "cell-839009d3ec9df5c5",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "F4hb674EjgKc"
      },
      "source": [
        "5. Plot the predictive distribution (both mean and standard deviation) for the following situations: maximum iterations in the variational inference algorithm set to 1, 2, 3, 5, 10."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "c4c0d87d72f1d66819a83fd933a187fc",
          "grade": true,
          "grade_id": "cell-91c5f95aab8a0fb4",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "z05y0me4jgKc"
      },
      "source": [
        "\"\"\"\n",
        "Plot predictive distributions.\n",
        "\"\"\"\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "0ec251d80109f1717fc50b6496cb6f62",
          "grade": false,
          "grade_id": "cell-be45dec324e5963f",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "tj7hj_uujgKd"
      },
      "source": [
        "Comment on the results, in particular on the speed of convergence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "5cd73b5f7c6a35758f4435c9236f5300",
          "grade": true,
          "grade_id": "cell-d0eb51c81a93821e",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "kdME1CJ8jgKe"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "abe25342d2d254506be4042fb18237d3",
          "grade": false,
          "grade_id": "cell-6f50b90c61f5a9a0",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "UGPBPweBjgKf"
      },
      "source": [
        "6. One of the quantities of interest in variational inference is the variational lower bound. The lower bound for Bayesian linear regression is given in Bishop in equations (10.107) - (10.112). Assume $a_{0}=0$ and $b_{0}=0$. Implement the function that computes the variational lower bound.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "b1859999c7c75c0dcaafb02480a60f82",
          "grade": true,
          "grade_id": "cell-5fbe5351946dbc6c",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "6SNZ99R0jgKf"
      },
      "source": [
        "def LB(X, t, M, beta, a0, b0, a, b, w_mean, w_var):\n",
        "        \"\"\"\n",
        "        Compute the variational lower bound.\n",
        "\n",
        "        Parameters\n",
        "        X : (N, M) np.ndarray\n",
        "            training independent variables\n",
        "        t : (N,) np.ndarray\n",
        "            training dependent variable\n",
        "        M : integer\n",
        "            order of the polynomial\n",
        "        beta : float\n",
        "            precision of observation noise (assumed to be known)\n",
        "        a0 : float\n",
        "            a parameter of the prior gamma distribution Gamma(alpha|a0,b0)\n",
        "        b0 : float\n",
        "            another parameter of the prior gamma distribution Gamma(alpha|a0,b0)\n",
        "        a : float\n",
        "            a parameter of the posterior gamma distribution Gamma(alpha|a,b)\n",
        "        b : float\n",
        "            another parameter of the posterior gamma distribution Gamma(alpha|a,b)\n",
        "\n",
        "        w_mean: float\n",
        "            the mean of the variational posterior distribution of the weights\n",
        "        w_var: float\n",
        "            the variance of the covariance posterior distribution of the weights\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        LB : lower bound\n",
        "        \"\"\"\n",
        "        # YOUR CODE HERE\n",
        "        raise NotImplementedError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "213574fdb55df58d297f179a8b6b0796",
          "grade": false,
          "grade_id": "cell-d2d2cb2f5739dd35",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "cvcDarT7jgKh"
      },
      "source": [
        "7. Produce a plot of the lower bound against different orders of the polynomial (see fig. 10.9 in Bishop as an example)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5821aa3116839946896e90da701e241c",
          "grade": true,
          "grade_id": "cell-81cb36e7dd7ece7c",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "M0zwU5mWjgKi"
      },
      "source": [
        "\"\"\"\n",
        "Plot the lower bound versus different orders of polynomials.\n",
        "\"\"\"\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "07ee6c0407600c27d33d6774f0c5480b",
          "grade": false,
          "grade_id": "cell-cc5b3901dc2be886",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "_8UWWCLEjgKi"
      },
      "source": [
        "How can you use the variational lower bound for model selection in a regression problem? How would it compare to the maximum likelihood solution?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "f69b3c491037dcf3f133a8d5ca6c557d",
          "grade": true,
          "grade_id": "cell-115163e9958d37c5",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "UllUEGhIjgKj"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "32ba3c062af4c8f3f7af9aa1b35aa0b3",
          "grade": false,
          "grade_id": "cell-df62a9f3cf0ac865",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "eefLywo9jgKk"
      },
      "source": [
        "8. What would we gain/lose by using MCMC (for example Gibbs sampling) for this problem instead of variational inference? For which types of problems is MCMC more suitable than VI? For which types of problems is VI more suitable than MCMC?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "73615dbd7c9bb6af5c701e548aaa8051",
          "grade": true,
          "grade_id": "cell-0622394098c48fdf",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "hHiu2XxIjgKk"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    }
  ]
}